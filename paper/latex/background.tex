\section{Background \& Summary}
\label{sec:background}

The advent of large-scale datasets tracing the workings of science has cultivated
a rapidly expanding ``science of science'' with its own data infrastructure,
metrics, and analytical frameworks~\cite{liu2023science}. Yet the databases that
underpin this field remain fragmented: Semantic Scholar provides influential
citation flags, open-access metadata, and full-text coverage for millions of
papers~\cite{kinney2023s2ag}; OpenAlex---the most comprehensive open index to
date---offers field-weighted citation impact (FWCI), a four-level topic taxonomy
with 4,516~leaf topics, geocoded institution affiliations for 121\,K
institutions, and 11.7~million funding awards with dollar amounts, all under a
CC0 license~\cite{priem2022openalex}; SciSciNet contributes a landmark
collection of pre-computed science-of-science metrics including disruption
indices, atypicality scores, and patent linkages~\cite{lin2023sciscinet}; Papers
with Code links papers to reproducible code, though the platform has ceased
active operations, making its archived snapshot a non-renewable
resource~\cite{paperswithcode2024}; Retraction Watch tracks integrity
events~\cite{retractionwatch2024}; and Reliance on Science maps patent-to-paper
citations~\cite{marx2020ros}. Each of these sources represents a substantial
achievement in opening the scientific record; however, no single source captures
all facets simultaneously. Researchers who wish to combine them---for example, to
study whether disruptive papers are more likely to release code, or whether
retracted papers show anomalous citation patterns across databases---must write
ad-hoc integration scripts that are rarely shared or reproduced.

A systematic evaluation of 59 scholarly databases found substantial variation in
backward and forward citation coverage~\cite{gusenbauer2024citation}. Large-scale
pairwise comparisons of bibliographic data sources have revealed non-trivial
differences in metadata, document types, and citation
counts~\cite{visser2021comparison}, but record-level joins across more than two
sources remain uncommon. The lack of a shared infrastructure forces each research
group to repeat the same data-wrangling steps, wasting effort and introducing
inconsistencies.

Several systems have begun to address this gap (Table~\ref{tab:comparison}).
SciSciNet~\cite{lin2023sciscinet} provides a rich ``data lake'' built on
Microsoft Academic Graph (now OpenAlex) with pre-computed science-of-science
metrics and linkages to patents, grants, and clinical trials, but its
bibliometric backbone draws from a single index and it does not preserve
independent source-level schemas for cross-source comparison.
PubGraph~\cite{ahrabian2023pubgraph} merges Wikidata, OpenAlex, and Semantic
Scholar into a unified knowledge graph using the Wikidata ontology, but
collapses source-level schemas, sacrificing the ability to compare how different
sources describe the same paper. SemOpenAlex~\cite{farber2023semopenalex}
re-encodes OpenAlex as 26~billion RDF triples, offering semantic-web
interoperability but remaining a single-source resource.
Dimensions~\cite{hook2018dimensions} provides SQL-queryable access to a
comprehensive commercial database, but its proprietary nature limits
reproducibility.

\begin{table}[t]
\centering
\caption{Comparison with existing scholarly data integration systems.}
\label{tab:comparison}
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabularx}{\textwidth}{@{} l c c c c X @{}}
\toprule
\textbf{System} & \textbf{Src} & \textbf{Multi} & \textbf{Schemas} & \textbf{Open} & \textbf{Key limitation} \\
\midrule
SciSciNet~\cite{lin2023sciscinet}        & 1+ & \texttimes & ---      & \checkmark & Single bibliometric backbone (MAG/OpenAlex); no independent cross-source comparison \\[2pt]
PubGraph~\cite{ahrabian2023pubgraph}     & 3  & \checkmark & Merged   & \checkmark & Collapses into unified Wikidata schema; source-level provenance lost \\[2pt]
SemOpenAlex~\cite{farber2023semopenalex} & 1  & \texttimes & ---      & \checkmark & Single source (OpenAlex only); requires SPARQL expertise \\[2pt]
Dimensions~\cite{hook2018dimensions}     & 1  & \texttimes & ---      & \texttildelow & Commercial; limited programmatic access; not fully reproducible \\[2pt]
\textbf{Science Data Lake}               & \textbf{8} & \checkmark & \textbf{Preserved} & \checkmark & Requires {\raise.17ex\hbox{$\scriptstyle\sim$}}1\,TB local storage; snapshot-based \\
\bottomrule
\end{tabularx}
\end{table}

Rather than attempting to replace these individually excellent resources, the
Science Data Lake provides an open infrastructure foundation for
science-of-science research---analogous to what
SciSciNet~\cite{lin2023sciscinet} achieved for single-source metrics, but
extended across eight independent data sources. The resource makes three
contributions. \textbf{First}, a \emph{multi-source preserving architecture}
that integrates eight open scholarly databases into a single resource while
retaining each source's native schema, enabling direct cross-source comparison at
the record level. Because upstream sources---particularly OpenAlex---evolve their
schema across snapshot partitions, the conversion pipeline employs dynamic Python
scripts that auto-discover entity types and column structures rather than relying
on hard-coded schemas, ensuring the infrastructure remains current as sources
change. \textbf{Second}, an \emph{embedding-based ontology alignment}
method that bridges OpenAlex's flat topic taxonomy to 13~formal scientific
ontologies using BGE-large sentence embeddings~\cite{xiao2024bge}, achieving a
17-fold improvement over string-matching baselines. \textbf{Third}, a
\emph{cross-source record-level comparison layer} (\texttt{unified\_papers},
293M rows, 29~columns) that enables simultaneous queries across all
sources---supporting analyses such as multi-database citation reliability
assessment that no single source or pairwise comparison can provide.
Beyond traditional research workflows, the SQL-native architecture and
structured documentation make the data lake particularly amenable to
emerging LLM-based research agents~\cite{wang2023aiforscience} that can
compose complex analytical queries from natural-language descriptions: a
machine-readable schema reference enables such agents to navigate the
151-view, 22-schema structure without prior domain knowledge.
